{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1nlJHJ52Q_a18cNVfu4ILkTfMEfgYQsD6","authorship_tag":"ABX9TyNFn0i9/jnEbEsRt/xWBW2c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KNd1we6vA5nx"},"outputs":[],"source":[]},{"cell_type":"code","source":["import librosa\n","import numpy as np\n","import soundfile as sf"],"metadata":{"id":"dtytj9VjBOrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_audio(audio_path='/content/sample_data/ERS/Audio'):\n","    audio, sr = librosa.load(audio_path, sr=None)\n","    return audio, sr\n"],"metadata":{"id":"vSYuBPcACTrh","executionInfo":{"status":"ok","timestamp":1732032353057,"user_tz":-330,"elapsed":491,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def extract_features(audio, sr):\n","    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n","    mfcc_mean = np.mean(mfccs, axis=1)\n","    return mfcc_mean"],"metadata":{"id":"4qYwCqzCLIPS","executionInfo":{"status":"ok","timestamp":1732032370232,"user_tz":-330,"elapsed":451,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["audio_files = []\n","labels = []"],"metadata":{"id":"5Wl5_EEjLLxw","executionInfo":{"status":"ok","timestamp":1732032383796,"user_tz":-330,"elapsed":636,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import os  # Import the os module\n","import glob\n","\n","# Define happy_audio_files using os.listdir or glob.glob\n","happy_audio_files = [f for f in glob.glob('/content/sample_data/ERS/Audio/Happy/*.wav')]  # Replace '*.wav' if your audio files have a different extension\n","# or\n","# happy_audio_files = os.listdir('/content/sample_data/ERS/Audio/Happy')\n","\n","for file in happy_audio_files:\n","    audio_files.append('/content/sample_data/ERS/Audio/Happy')\n","    labels.append(1)  # 0 for happy\n","# Similarly, for sad and angry audio files"],"metadata":{"id":"3r9jRZ_8LU84","executionInfo":{"status":"ok","timestamp":1732032748157,"user_tz":-330,"elapsed":413,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import os  # Import the os module\n","import glob\n","\n","# Define happy_audio_files using os.listdir or glob.glob\n","sad_audio_files = [f for f in glob.glob('/content/sample_data/ERS/Audio/Sad/*.wav')]  # Replace '*.wav' if your audio files have a different extension\n","# or\n","# happy_audio_files = os.listdir('/content/sample_data/ERS/Audio/Sad')\n","\n","for file in happy_audio_files:\n","    audio_files.append('/content/sample_data/ERS/Audio/Sad')\n","    labels.append(2)  # 0 for happy\n","# Similarly, for sad and angry audio files"],"metadata":{"id":"YtaIUGKvMHEp","executionInfo":{"status":"ok","timestamp":1732032745694,"user_tz":-330,"elapsed":447,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import os  # Import the os module\n","import glob\n","\n","# Define happy_audio_files using os.listdir or glob.glob\n","angry_audio_files = [f for f in glob.glob('/content/sample_data/ERS/Audio/Angry/*.wav')]  # Replace '*.wav' if your audio files have a different extension\n","# or\n","# happy_audio_files = os.listdir('/content/sample_data/ERS/Audio/Angry')\n","\n","for file in happy_audio_files:\n","    audio_files.append('/content/sample_data/ERS/Audio/Angry')\n","    labels.append(3)  # 0 for happy\n","# Similarly, for sad and angry audio files"],"metadata":{"id":"6jI-XAwBMU_Y","executionInfo":{"status":"ok","timestamp":1732032751926,"user_tz":-330,"elapsed":431,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import numpy as np # Import numpy with alias np\n","import librosa\n","import numpy as np # Import numpy with alias np\n","import librosa\n","import librosa.display\n","\n","def load_audio(audio_file_path):\n","    signal, sr = librosa.load(audio_file_path)\n","    return signal, sr\n","\n","def extract_features(signal, sr):\n","    # Example features\n","    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n","    mfccs_processed = np.mean(mfccs.T, axis=0)\n","    return mfccs_processed\n","\n","audio_files = []\n","labels = []\n","\n","# ... (Rest of your code to populate audio_files and labels) ...\n","\n","X = np.array([extract_features(load_audio(file)[0], load_audio(file)[1]) for file in audio_files])\n","y = np.array(labels)\n","\n","\n","def extract_features(signal, sr):\n","    # Example features\n","    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n","    mfccs_processed = np.mean(mfccs.T, axis=0)\n","    return mfccs_processed\n","\n","audio_files = []\n","labels = []\n","\n","# ... (Rest of your code to populate audio_files and labels) ...\n","\n","X = np.array([extract_features(load_audio(file)[0], load_audio(file)[1]) for file in audio_files])\n","y = np.array(labels)"],"metadata":{"id":"d1QxZGEQMnix","executionInfo":{"status":"ok","timestamp":1732032898266,"user_tz":-330,"elapsed":470,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"GwQYupGKMqsg","executionInfo":{"status":"ok","timestamp":1732032915342,"user_tz":-330,"elapsed":2201,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import os  # Import the os module\n","import glob\n","import numpy as np # Import numpy with alias np\n","import librosa\n","import librosa.display\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","def load_audio(audio_file_path):\n","    signal, sr = librosa.load(audio_file_path)\n","    return signal, sr\n","\n","def extract_features(signal, sr):\n","    # Example features\n","    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n","    mfccs_processed = np.mean(mfccs.T, axis=0)\n","    return mfccs_processed\n","\n","audio_files = []\n","labels = []\n","\n","# Load Happy audio files\n","happy_audio_files = [f for f in glob.glob('/content/sample_data/ERS/Audio/Happy/*.m4a')]  # Replace '*.wav' with the actual extension if different\n","for file in happy_audio_files:\n","    audio_files.append(file)  # Append the file path, not the directory\n","    labels.append(0)  # 0 for happy\n","\n","# Load Sad audio files\n","sad_audio_files = [f for f in glob.glob('/content/sample_data/ERS/Audio/Sad/*.m4a')]  # Replace '*.wav' with the actual extension if different\n","for file in sad_audio_files:\n","    audio_files.append(file)  # Append the file path, not the directory\n","    labels.append(2)  # 2 for sad\n","\n","# Load Angry audio files\n","angry_audio_files = [f for f in glob.glob('/content/sample_data/ERS/Audio/Angry/*.m4a')]  # Replace '*.wav' with the actual extension if different\n","for file in angry_audio_files:\n","    audio_files.append(file)  # Append the file path, not the directory\n","    labels.append(3)  # 3 for angry\n","\n","\n","X = np.array([extract_features(load_audio(file)[0], load_audio(file)[1]) for file in audio_files])\n","y = np.array(labels)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7FnQHD-NSVx","executionInfo":{"status":"ok","timestamp":1732033092761,"user_tz":-330,"elapsed":22216,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}},"outputId":"bd3717f1-46fb-43c5-8d30-5067e5c6bac0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-19-891bb53e2172>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  signal, sr = librosa.load(audio_file_path)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]}]},{"cell_type":"code","source":["model = SVC()\n","model.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"W1GA9Wn2N7h5","executionInfo":{"status":"ok","timestamp":1732033116619,"user_tz":-330,"elapsed":476,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}},"outputId":"b0138762-371d-4cb2-d363-fed7b79bc1b8"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC()"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["y_pred = model.predict(X_test)"],"metadata":{"id":"1f0dagvPOFMS","executionInfo":{"status":"ok","timestamp":1732033144717,"user_tz":-330,"elapsed":707,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JVuPzUoOHB4","executionInfo":{"status":"ok","timestamp":1732033159243,"user_tz":-330,"elapsed":463,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}},"outputId":"cbd202fc-0771-4437-8ecf-262bcfb7b442"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.6666666666666666\n"]}]},{"cell_type":"code","source":["import os\n","import pickle\n","\n","# ... (Your existing code to load audio, extract features, and split data) ...\n","import os\n","import pickle\n","\n","# ... (Your existing code to load audio, extract features, and split data) ...\n","\n","# Define the folder path where you want to save the pickle file\n","folder_path = r\"C:\\Users\\DELL\\Desktop\\Emotion Recognition\"  # Replace with your actual folder path\n","\n","# Create the folder if it doesn't exist\n","os.makedirs(folder_path, exist_ok=True)\n","\n","# Define the file name for the pickle file\n","file_name = \"Emotion_recognition_project.pkl\"\n","\n","# Create the full file path\n","file_path = os.path.join(folder_path, file_name)\n","\n","# Create a dictionary to store the data you want to save\n","data_to_save = {\n","    \"X_train\": X_train,\n","    \"X_test\": X_test,\n","    \"y_train\": y_train,\n","    \"y_test\": y_test,\n","    # Add any other data you want to save, like feature names, etc.\n","}\n","\n","# Save the data to the pickle file\n","with open(file_path, \"wb\") as file:\n","    pickle.dump(data_to_save, file)\n","\n","print(f\"Project data saved to: {file_path}\")\n","\n","os.makedirs(folder_path, exist_ok=True)\n","\n","# Define the file name for the pickle file\n","file_name = \"Emotion_recognition_project.pkl\"\n","\n","# Create the full file path\n","file_path = os.path.join(folder_path, file_name)\n","\n","# Create a dictionary to store the data you want to save\n","data_to_save = {\n","    \"X_train\": X_train,\n","    \"X_test\": X_test,\n","    \"y_train\": y_train,\n","    \"y_test\": y_test,\n","    # Add any other data you want to save, like feature names, etc.\n","}\n","\n","# Save the data to the pickle file\n","with open(file_path, \"wb\") as file:\n","    pickle.dump(data_to_save, file)\n","\n","print(f\"Project data saved to: {file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUQg4r0vOK7Q","executionInfo":{"status":"ok","timestamp":1732033735427,"user_tz":-330,"elapsed":495,"user":{"displayName":"NEER RAICHURA","userId":"13098098889713800332"}},"outputId":"86a7274d-69df-4af8-a987-d7db583bc914"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Project data saved to: C:\\Users\\DELL\\Desktop\\Emotion Recognition/Emotion_recognition_project.pkl\n","Project data saved to: C:\\Users\\DELL\\Desktop\\Emotion Recognition/Emotion_recognition_project.pkl\n"]}]}]}